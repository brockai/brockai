{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use tokenizer.Trainers BpeTrainer to produce trained tokenizer (tokenizer-main.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import csv into panda dataframe\n",
    "df = pd.read_csv(\"../assets/main.csv\").fillna(\"[UNK]\")\n",
    "df_list = {'text':df.values.tolist()}\n",
    "\n",
    "tokenizer.train_from_iterator(df_list, trainer)\n",
    "\n",
    "tokenizer.save(\"../assets/tokenizer-main.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files=\"../assets/test.csv\")\n",
    "\n",
    "print(dataset)\n",
    "dataset.push_to_hub(\"brockai/rumzer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# create train, test, unsupervised datasets in DatasetDict()\n",
    "data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\", \"unsupervised\": \"unsupervised.csv\"}\n",
    "ds = load_dataset(\"brockai/rumzer\", data_files=data_files, revision=\"main\")\n",
    "\n",
    "ds.head()\n",
    "# ds[\"train\"].features\n",
    "print(ds[\"train\"].features)\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# def tokenize(examples):\n",
    "#     outputs = tokenizer(examples['text'], truncation=True)\n",
    "#     return outputs\n",
    "\n",
    "# tokenized_ds = ds.map(tokenize, batched=True)\n",
    "\n",
    "# unsupervised_dataset\n",
    "# train_dataset, test_dataset = rumzer.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "# print(\"Training Dataset:\")\n",
    "# print(train_dataset)\n",
    "# print(\"\\nTesting Dataset:\")\n",
    "# print(test_dataset)\n",
    "\n",
    "# from datasets import load_dataset_builder\n",
    "# ds_builder = load_dataset_builder(\"brockai/rumzer\")\n",
    "\n",
    "# from datasets import get_dataset_split_names\n",
    "# get_dataset_split_names('brockai/rumzer')\n",
    "\n",
    "# # print(rumzer)\n",
    "# # print(imdb)\n",
    "# print(ds_builder.info.description)\n",
    "# print(ds_builder.info.features)\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# # tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "\n",
    "# from transformers import DataCollatorWithPadding\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "# label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# import csv into panda dataframe\n",
    "# df = pd.read_csv(\"../assets/main.csv\").fillna(\"[UNK]\")\n",
    "\n",
    "from datasets import load_dataset_builder\n",
    "# ds_builder = load_dataset_builder(\"brockai/rumzer\")\n",
    "# print(ds_builder)\n",
    "ds = load_dataset(\"brockai/rumzer\", split=\"train\")\n",
    "ds = ds.map(lambda x: tokenizer(x['Description'], truncation=True, padding=True), batched=True)\n",
    "\n",
    "ds.set_format(columns=[\"Description\",\"OtherIDs\",\"Suppliers\",\"Manufacturers\",\"Policy\",\"Declarables\",\"Judgement\"])\n",
    "\n",
    "# dataset.set_format(type='pandas', columns=[\"Description\",\"OtherIDs\",\"Suppliers\",\"Manufacturers\",\"Policy\",\"Declarables\",\"Judgement\"])\n",
    "print(ds)\n",
    "print(ds.format)\n",
    "# df = dataset.to_pandas()\n",
    "\n",
    "# df = df[[\"Description\",\"OtherIDs\",\"Suppliers\",\"Manufacturers\",\"Policy\",\"Declarables\",\"Judgement\"]]\n",
    "\n",
    "# dataset = load_dataset('csv', data_files=\"../assets/main.csv\")\n",
    "# dataset = load_dataset(\"glue\", \"brockai/rumzer\", split=\"train\")\n",
    "\n",
    "# print(dataset)\n",
    "# dataset.push_to_hub(dataset)\n",
    "\n",
    "data_dict = {'Description': [], 'OtherIDs': [], 'Suppliers': [], 'Manufacturers': [], 'Policy': [], 'Declarables': [], 'Judgement': []}\n",
    "\n",
    "# keys = df.keys()\n",
    "keys = ds.column_names\n",
    "\n",
    "print(keys)\n",
    "\n",
    "# walk the dataframe, build tensor for each column in a row\n",
    "for i in range(len(ds)):\n",
    "    for key in keys:\n",
    "        print(ds[i][key])\n",
    "        # t_token = tokenizer(ds[i][key], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        # data_dict[key].append(t_token)\n",
    "\n",
    "# add tensors to dataframe\n",
    "# for key in keys:\n",
    "#   df['tokenized_'+key] = data_dict[key]\n",
    "\n",
    "# df.head()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
